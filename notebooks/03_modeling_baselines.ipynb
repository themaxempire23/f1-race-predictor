{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448b5e02-fb3b-4a4a-a786-a440d1f65fdc",
   "metadata": {},
   "source": [
    "## Load features, select columns, prep targets\n",
    "\n",
    "- Goal: load enriched data and select a minimal, stable feature set.\n",
    "- We’ll start small with interpretable features and fill small gaps with medians.\n",
    "- Targets: finish_pos (regression) and top10 (classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407e1464-37cb-4d44-8c61-caa40bc6acf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['fp_mean_all_s', 'fp_median_longrun_s', 'fp_total_laps', 'qual_pos', 'delta_to_pole_s', 'grid_pos', 'grid_drop', 'roll3_avg_pos', 'roll5_avg_pos', 'roll3_pts', 'roll5_pts', 'roll5_dnf_rate', 'team_roll3_pts', 'team_roll5_pts', 'track_hist_avg_pos']\n",
      "Rows: 200 | groups: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "PROCESSED = ROOT / \"data\" / \"processed\"\n",
    "df = pd.read_csv(PROCESSED / \"season_2023_features_enriched.csv\")\n",
    "\n",
    "if \"grid_drop\" not in df.columns and {\"grid_pos\",\"qual_pos\"}.issubset(df.columns):\n",
    "    df[\"grid_drop\"] = df[\"grid_pos\"] - df[\"qual_pos\"]\n",
    "\n",
    "features = [c for c in [\n",
    "    \"fp_mean_all_s\",\"fp_median_longrun_s\",\"fp_total_laps\",\n",
    "    \"qual_pos\",\"delta_to_pole_s\",\"grid_pos\",\"grid_drop\",\n",
    "    \"roll3_avg_pos\",\"roll5_avg_pos\",\"roll3_pts\",\"roll5_pts\",\"roll5_dnf_rate\",\n",
    "    \"team_roll3_pts\",\"team_roll5_pts\",\"track_hist_avg_pos\",\n",
    "] if c in df.columns]\n",
    "\n",
    "X = df[features].fillna(df[features].median(numeric_only=True))\n",
    "y_reg = df[\"finish_pos\"].astype(float)\n",
    "y_cls = df[\"top10\"].astype(int)\n",
    "groups = df[\"group_key\"]\n",
    "print(\"Features:\", features)\n",
    "print(\"Rows:\", X.shape[0], \"| groups:\", groups.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907cc81-a9b1-4a5d-b1ad-9ebda924b81d",
   "metadata": {},
   "source": [
    "## Diagnose missing & constant features\n",
    "\n",
    "- Goal: see where the NaNs/∞ and constants are before modeling.\n",
    "- This shows which columns have missing values and which are constant (no variance).\n",
    "- Constant or all-NaN columns don’t help models and can break scalers/imputers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc187126-68ba-4f52-9af4-cb29f162d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top NaN counts:\n",
      " track_hist_avg_pos     200\n",
      "fp_median_longrun_s      0\n",
      "fp_mean_all_s            0\n",
      "qual_pos                 0\n",
      "delta_to_pole_s          0\n",
      "grid_pos                 0\n",
      "fp_total_laps            0\n",
      "grid_drop                0\n",
      "roll3_avg_pos            0\n",
      "roll3_pts                0\n",
      "dtype: int64\n",
      "Constant columns: ['roll5_dnf_rate', 'track_hist_avg_pos'] … total: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nan_counts = X.isna().sum().sort_values(ascending=False)\n",
    "const_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "print(\"Top NaN counts:\\n\", nan_counts.head(10))\n",
    "print(\"Constant columns:\", const_cols[:10], \"… total:\", len(const_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b62dc-7b56-429b-bfcd-6d030c924d17",
   "metadata": {},
   "source": [
    "## Cleaning X: drop all-NaN & constant cols, replace ±∞\n",
    "\n",
    "- Goal: make features model-friendly; keep column list for later.\n",
    "- We safely remove unusable columns so the imputer/scaler has real data to work with.\n",
    "- We keep the final feature list to use for training and any coefficient peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3327a942-0ee3-4481-9464-166fc0a89349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping all-NaN columns: ['track_hist_avg_pos']\n",
      "Dropping constant columns: ['roll5_dnf_rate']\n",
      "Kept 13 features.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "Xc = X.replace([np.inf, -np.inf], np.nan).copy()\n",
    "\n",
    "# Drop columns that are entirely NaN\n",
    "all_nan_cols = Xc.columns[Xc.isna().all()]\n",
    "if len(all_nan_cols):\n",
    "    print(\"Dropping all-NaN columns:\", list(all_nan_cols))\n",
    "    Xc = Xc.drop(columns=list(all_nan_cols))\n",
    "\n",
    "# Drop columns that are constant (no variance)\n",
    "const_cols = [c for c in Xc.columns if Xc[c].nunique(dropna=True) <= 1]\n",
    "if len(const_cols):\n",
    "    print(\"Dropping constant columns:\", const_cols)\n",
    "    Xc = Xc.drop(columns=const_cols)\n",
    "\n",
    "features_clean = Xc.columns.tolist()\n",
    "print(\"Kept\", len(features_clean), \"features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927dc89f-33d2-4c2e-853f-10aa940eb95e",
   "metadata": {},
   "source": [
    "## GroupKFold baselines\n",
    "\n",
    "- Goal: add SimpleImputer(median) so Linear/Logistic can handle missing values.\n",
    "- SimpleImputer(median) resolves the NaN error; the scaler handles different scales.\n",
    "- We print concise metrics for both tasks using GroupKFold (no weekend leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70cd4c9f-8d40-49c3-b18a-03cdea2bfddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression — MAE: 2.28 | Spearman: -0.071\n",
      "Classification — Acc: 0.86 | F1: 0.86 | Brier: 0.099\n"
     ]
    }
   ],
   "source": [
    "# Goal: add SimpleImputer(median) so Linear/Logistic can handle missing values.\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score\n",
    "\n",
    "def spearman_like(y_true, y_pred):\n",
    "    a = pd.Series(y_true).rank()\n",
    "    b = pd.Series(y_pred).rank()\n",
    "    return a.corr(b)\n",
    "\n",
    "gkf = GroupKFold(n_splits=min(5, groups.nunique()))\n",
    "mae, spr, acc, f1, brier = [], [], [], [], []\n",
    "\n",
    "for tr, te in gkf.split(Xc, y_reg, groups):\n",
    "    Xtr, Xte = Xc.iloc[tr], Xc.iloc[te]\n",
    "    ytr_r, yte_r = y_reg.iloc[tr], y_reg.iloc[te]\n",
    "    ytr_c, yte_c = y_cls.iloc[tr], y_cls.iloc[te]\n",
    "\n",
    "    # Regression (imputer + scaler + linear reg)\n",
    "    reg = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                        StandardScaler(with_mean=False),\n",
    "                        LinearRegression())\n",
    "    reg.fit(Xtr, ytr_r)\n",
    "    pred_r = reg.predict(Xte)\n",
    "    mae.append(mean_absolute_error(yte_r, pred_r))\n",
    "    spr.append(spearman_like(yte_r, pred_r))\n",
    "\n",
    "    # Classification (imputer + scaler + logistic)\n",
    "    clf = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                        StandardScaler(with_mean=False),\n",
    "                        LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "    clf.fit(Xtr, ytr_c)\n",
    "    proba = clf.predict_proba(Xte)[:, 1]\n",
    "    pred_c = (proba >= 0.5).astype(int)\n",
    "    acc.append(accuracy_score(yte_c, pred_c))\n",
    "    f1.append(f1_score(yte_c, pred_c))\n",
    "    brier.append(np.mean((proba - yte_c.values)**2))\n",
    "\n",
    "print(f\"Regression — MAE: {np.mean(mae):.2f} | Spearman: {np.mean(spr):.3f}\")\n",
    "print(f\"Classification — Acc: {np.mean(acc):.2f} | F1: {np.mean(f1):.2f} | Brier: {np.mean(brier):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2deebd-08b2-4ae1-bf6a-291e15aa91f5",
   "metadata": {},
   "source": [
    "## Refit on all data & view coefficients\n",
    "\n",
    "- Goal: fit one final regression on all rows and inspect coefficients\n",
    "- This gives a rough sense of which features move predicted finishing position.\n",
    "- We’ll add tree-based importances in the “stronger models” step next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0fba8c-f957-4a6a-bf20-a23b5c2de664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top +ve:\n",
      " qual_pos          0.573772\n",
      "roll5_pts         0.680658\n",
      "team_roll5_pts    1.029664\n",
      "roll5_avg_pos     1.330264\n",
      "roll3_avg_pos     3.138200\n",
      "dtype: float64 \n",
      "\n",
      "Top -ve:\n",
      " team_roll3_pts   -1.180159\n",
      "roll3_pts        -0.232500\n",
      "fp_mean_all_s    -0.161033\n",
      "grid_drop        -0.148764\n",
      "fp_total_laps    -0.000673\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "final_reg = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                          StandardScaler(with_mean=False),\n",
    "                          LinearRegression())\n",
    "final_reg.fit(Xc, y_reg)\n",
    "\n",
    "coefs = pd.Series(final_reg.named_steps[\"linearregression\"].coef_, index=features_clean).sort_values()\n",
    "print(\"Top +ve:\\n\", coefs.tail(5), \"\\n\\nTop -ve:\\n\", coefs.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5f296-fb39-4711-850e-344f9cfe126a",
   "metadata": {},
   "source": [
    "## RandomForest (regression) with small param sweep + GroupKFold\n",
    "\n",
    "- Goal: try a tiny RF regressor grid with GroupKFold and pick the best by MAE.\n",
    "- We evaluate a tiny RF grid with GroupKFold and choose the params that minimize MAE.\n",
    "- Spearman checks ranking quality; we keep the best config for final fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d97307-4fd7-49b2-b986-25426313537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF-Reg {'n_estimators': 300, 'max_depth': None, 'min_samples_leaf': 1} → MAE: 2.34 | Spearman: -0.108\n",
      "RF-Reg {'n_estimators': 300, 'max_depth': 12, 'min_samples_leaf': 2} → MAE: 2.33 | Spearman: -0.102\n",
      "RF-Reg {'n_estimators': 500, 'max_depth': 16, 'min_samples_leaf': 2} → MAE: 2.31 | Spearman: -0.114\n",
      "Best RF-Reg: {'n_estimators': 500, 'max_depth': 16, 'min_samples_leaf': 2} | MAE: 2.31 | Spearman: -0.114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "param_grid = [\n",
    "    {\"n_estimators\": 300, \"max_depth\": None, \"min_samples_leaf\": 1},\n",
    "    {\"n_estimators\": 300, \"max_depth\": 12,   \"min_samples_leaf\": 2},\n",
    "    {\"n_estimators\": 500, \"max_depth\": 16,   \"min_samples_leaf\": 2},\n",
    "]\n",
    "\n",
    "gkf = GroupKFold(n_splits=min(5, groups.nunique()))\n",
    "best, best_mae, best_spr = None, 1e9, None\n",
    "\n",
    "def spearman_like(y_true, y_pred):\n",
    "    a, b = pd.Series(y_true).rank(), pd.Series(y_pred).rank()\n",
    "    return a.corr(b)\n",
    "\n",
    "for p in param_grid:\n",
    "    maes, sprs = [], []\n",
    "    for tr, te in gkf.split(Xc, y_reg, groups):\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        Xtr, Xte = imputer.fit_transform(Xc.iloc[tr]), imputer.transform(Xc.iloc[te])\n",
    "        ytr, yte = y_reg.iloc[tr], y_reg.iloc[te]\n",
    "        rf = RandomForestRegressor(random_state=42, n_jobs=-1, **p)\n",
    "        rf.fit(Xtr, ytr)\n",
    "        pred = rf.predict(Xte)\n",
    "        maes.append(mean_absolute_error(yte, pred))\n",
    "        sprs.append(spearman_like(yte, pred))\n",
    "    m_mae, m_spr = float(np.mean(maes)), float(np.mean(sprs))\n",
    "    print(\"RF-Reg\", p, \"→ MAE:\", f\"{m_mae:.2f}\", \"| Spearman:\", f\"{m_spr:.3f}\")\n",
    "    if m_mae < best_mae:\n",
    "        best, best_mae, best_spr = p, m_mae, m_spr\n",
    "\n",
    "print(\"Best RF-Reg:\", best, \"| MAE:\", f\"{best_mae:.2f}\", \"| Spearman:\", f\"{best_spr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942859e-599b-4361-8afd-da336760085d",
   "metadata": {},
   "source": [
    "## RandomForest (classification) + GroupKFold (Acc/F1/Brier)\n",
    "\n",
    "- Goal: tiny RF classifier grid with GroupKFold, pick best by Brier (prob quality).\n",
    "- We select the classifier by lowest Brier (best-calibrated probabilities), and report Acc/F1 too.\n",
    "This gives us a stronger points-probability model for simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343039d4-65a4-4864-a568-9f802d1ca0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF-Cls {'n_estimators': 300, 'max_depth': None, 'min_samples_leaf': 1} → Acc: 0.84 | F1: 0.84 | Brier: 0.114\n",
      "RF-Cls {'n_estimators': 300, 'max_depth': 10, 'min_samples_leaf': 2} → Acc: 0.83 | F1: 0.83 | Brier: 0.111\n",
      "RF-Cls {'n_estimators': 500, 'max_depth': 14, 'min_samples_leaf': 2} → Acc: 0.83 | F1: 0.83 | Brier: 0.111\n",
      "Best RF-Cls: {'n_estimators': 500, 'max_depth': 14, 'min_samples_leaf': 2} | Acc: 0.83 | F1: 0.83 | Brier: 0.111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "param_grid_cls = [\n",
    "    {\"n_estimators\": 300, \"max_depth\": None, \"min_samples_leaf\": 1},\n",
    "    {\"n_estimators\": 300, \"max_depth\": 10,   \"min_samples_leaf\": 2},\n",
    "    {\"n_estimators\": 500, \"max_depth\": 14,   \"min_samples_leaf\": 2},\n",
    "]\n",
    "\n",
    "gkf = GroupKFold(n_splits=min(5, groups.nunique()))\n",
    "best_c, best_brier, best_tuple = None, 1e9, None\n",
    "\n",
    "for p in param_grid_cls:\n",
    "    accs, f1s, briers = [], [], []\n",
    "    for tr, te in gkf.split(Xc, y_cls, groups):\n",
    "        imp = SimpleImputer(strategy=\"median\")\n",
    "        Xtr, Xte = imp.fit_transform(Xc.iloc[tr]), imp.transform(Xc.iloc[te])\n",
    "        ytr, yte = y_cls.iloc[tr], y_cls.iloc[te]\n",
    "        rf = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=None, **p)\n",
    "        rf.fit(Xtr, ytr)\n",
    "        proba = rf.predict_proba(Xte)[:, 1]\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "        accs.append(accuracy_score(yte, pred))\n",
    "        f1s.append(f1_score(yte, pred))\n",
    "        briers.append(np.mean((proba - yte.values)**2))\n",
    "    m_acc, m_f1, m_brier = float(np.mean(accs)), float(np.mean(f1s)), float(np.mean(briers))\n",
    "    print(\"RF-Cls\", p, \"→ Acc:\", f\"{m_acc:.2f}\", \"| F1:\", f\"{m_f1:.2f}\", \"| Brier:\", f\"{m_brier:.3f}\")\n",
    "    if m_brier < best_brier:\n",
    "        best_c, best_brier, best_tuple = p, m_brier, (m_acc, m_f1)\n",
    "\n",
    "print(\"Best RF-Cls:\", best_c, \"| Acc:\", f\"{best_tuple[0]:.2f}\", \"| F1:\", f\"{best_tuple[1]:.2f}\", \"| Brier:\", f\"{best_brier:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4b93b-6596-4f90-891b-290903c216a3",
   "metadata": {},
   "source": [
    "## Fit best RFs on all data and save\n",
    "\n",
    "- Goal:refit best RF models on all rows and save them for reuse.\n",
    "- We train both best models on all data and save them to /models for reuse.\n",
    "- These will power the Monte Carlo simulation in the next phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13500c88-1a67-4c38-9443-7e61ddf008f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: rf_finishpos.joblib and rf_top10.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "\n",
    "MODELS = (Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()) / \"models\"\n",
    "MODELS.mkdir(exist_ok=True)\n",
    "\n",
    "# Refit regression\n",
    "imp_reg = SimpleImputer(strategy=\"median\")\n",
    "rf_reg  = RandomForestRegressor(random_state=42, n_jobs=-1, **best)\n",
    "reg_pipe = Pipeline([(\"imp\", imp_reg), (\"rf\", rf_reg)])\n",
    "reg_pipe.fit(Xc, y_reg)\n",
    "dump(reg_pipe, MODELS / \"rf_finishpos.joblib\")\n",
    "\n",
    "# Refit classification\n",
    "imp_cls = SimpleImputer(strategy=\"median\")\n",
    "rf_cls  = RandomForestClassifier(random_state=42, n_jobs=-1, **best_c)\n",
    "cls_pipe = Pipeline([(\"imp\", imp_cls), (\"rf\", rf_cls)])\n",
    "cls_pipe.fit(Xc, y_cls)\n",
    "dump(cls_pipe, MODELS / \"rf_top10.joblib\")\n",
    "\n",
    "print(\"Saved:\", (MODELS / 'rf_finishpos.joblib').name, \"and\", (MODELS / 'rf_top10.joblib').name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (f1-predictor)",
   "language": "python",
   "name": "f1-predictor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
